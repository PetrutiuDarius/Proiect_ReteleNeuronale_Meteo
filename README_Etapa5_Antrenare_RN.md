# ğŸ“˜ README â€“ Etapa 5: Configurarea È™i antrenarea modelului RN (Time Series)

**Disciplina:** ReÈ›ele Neuronale  
**InstituÈ›ie:** POLITEHNICA BucureÈ™ti â€“ FIIR  
**Student:** PetruÈ›iu Darius-Simion  
**Link Repository GitHub:** https://github.com/PetrutiuDarius/Proiect_ReteleNeuronale_Meteo.git  
**Data:** 11.12.2025  

---

## Scopul etapei 5

AceastÄƒ etapÄƒ corespunde punctului **6. Configurarea È™i antrenarea modelului RN** din lista de 9 etape.

**Obiectiv principal:** Transformarea scheletului software din etapa 4 Ã®ntr-un sistem inteligent complet funcÈ›ional. Acest lucru s-a realizat prin antrenarea efectivÄƒ a modelului LSTM pe setul de date hibrid (istoric real + scenarii "Black Swan"), evaluarea performanÈ›ei acestuia È™i integrarea modelului antrenat ("inference-ready") Ã®n dashboard-ul interactiv.

**Pornire obligatorie (status actual al proiectului):**

Dezvoltarea a pornit de la arhitectura validatÄƒ anterior, avÃ¢nd asigurate urmÄƒtoarele precondiÈ›ii:

* **State machine definit:** Fluxul de monitorizare ciclicÄƒ cu gestionare de evenimente este implementat Ã®n cod È™i documentat vizual Ã®n `docs/state-machine-RN.png`.
* **Cele 3 module funcÈ›ionale:**
    1.  **Data Logging:** Pipeline-ul genereazÄƒ È™i preproceseazÄƒ automat datele, incluzÃ¢nd parametrul critic de **precipitaÈ›ii**.
    2.  **Neural Network:** Arhitectura LSTM definitÄƒ Ã®n etapa 4 a fost instanÈ›iatÄƒ È™i antrenatÄƒ efectiv (fiÈ™ierul `models/trained_model.keras` este acum un artefact activ).
    3.  **UI (interfaÈ›a):** Dashboard-ul `src/app/dashboard.py` este conectat la modelul antrenat È™i afiÈ™eazÄƒ predicÈ›ii bazate pe date reale, nu valori simulate.
* **ContribuÈ›ia originalÄƒ:** dataset-ul utilizat pentru antrenare conÈ›ine **40% date originale** generate prin simulare fizicÄƒ (scenarii de caniculÄƒ, furtunÄƒ È™i Ã®ngheÈ›), asigurÃ¢nd expunerea modelului la situaÈ›ii extreme.

---

## 1. PregÄƒtirea datelor pentru antrenare

Deoarece Ã®n etapa 4 am integrat datele sintetice ("Black Swan"), dataset-ul de antrenare a suferit modificÄƒri structurale majore (creÈ™terea volumului cu >40%). Pentru a asigura consistenÈ›a antrenÄƒrii, am refÄƒcut preprocesarea pe setul hibrid.

### 1.1. Regenerarea È™i combinarea datelor
Am utilizat generatorul sintetic pentru a crea fiÈ™ierul consolidat `hybrid_dataset.csv`.

```bash
# ComandÄƒ rulatÄƒ pentru generarea dataset-ului hibrid:
python src/data_acquisition/synthetic_generator.py

```

*Rezultat:* FiÈ™ierul `data/generated/hybrid_dataset.csv` conÈ›ine acum atÃ¢t istoricul real (2020-2024), cÃ¢t È™i scenariile extreme (caniculÄƒ, furtunÄƒ, Ã®ngheÈ›).

### 1.2\. Preprocesarea completÄƒ (refÄƒcutÄƒ)

Am rulat scriptul de splitting È™i scalare pe noul dataset hibrid pentru a actualiza parametrii scaler-ului (MinMax) astfel Ã®ncÃ¢t sÄƒ includÄƒ È™i valorile extreme generate (ex: temperaturi de 44Â°C, care depÄƒÈ™esc maximul istoric).

Bash

```
# ComandÄƒ rulatÄƒ pentru preprocesare:
python src/preprocessing/split_data.py

```

**VerificÄƒri efectuate:**

-   [x] **ConsistenÈ›a scaler-ului:** FiÈ™ierul `config/preprocessing_params.pkl` a fost suprascris cu noile limite [Min, Max] ale setului hibrid.

-   [x] **Stratificare temporalÄƒ:**

    -   `data/train/train.csv`: ConÈ›ine date istorice (2020-2023) + **toate datele sintetice**.

    -   `data/validation/validation.csv`: Date reale 2024 (luni impare) - nealterate sintetic.

    -   `data/test/test.csv`: Date reale 2024 (luni pare) - nealterate sintetic.

    -   *Motiv:* ValidÄƒm È™i testÄƒm modelul pe date exclusiv reale pentru a mÄƒsura performanÈ›a Ã®n condiÈ›ii naturale, dar antrenÄƒm pe date augmentate pentru robusteÈ›e.

**Verificare rapidÄƒ a volumului de date:**

Python

```
import pandas as pd
train = pd.read_csv('data/train/train.csv')
print(f"Total Train Samples: {len(train)}")
# Output aÈ™teptat: ~60,000+ (Include cele 25,000 sintetice)
```

---

## 2. ConfiguraÈ›ia experimentului È™i arhitectura reÈ›elei

Pentru a modela dinamica complexÄƒ a parametrilor meteorologici, am proiectat o arhitecturÄƒ bazatÄƒ pe reÈ›ele recurente **LSTM (Long Short-Term Memory)**. Aceasta a fost aleasÄƒ pentru capacitatea sa de a Ã®nvÄƒÈ›a dependenÈ›e pe termen lung Ã®n serii temporale, fiind superioarÄƒ reÈ›elelor dense (feed-forward) care ignorÄƒ secvenÈ›ialitatea datelor.

### 2.1 Hiperparametrii selectaÈ›i
ConfiguraÈ›ia finalÄƒ a modelului a fost stabilitÄƒ Ã®n fiÈ™ierul `src/config.py` Ã®n urma experimentelor iterative, avÃ¢nd ca scop echilibrul Ã®ntre capacitatea de Ã®nvÄƒÈ›are È™i generalizare.

| Hiperparametru          | Valoare                                        | Justificare tehnicÄƒ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|:------------------------|:-----------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **ArhitecturÄƒ**         | **Stacked LSTM** (2 straturi: 64 + 32 unitÄƒÈ›i) | S-a optat pentru o arhitecturÄƒ ierarhicÄƒ ("Stacked") pentru a modela complexitatea fizicÄƒ a atmosferei. **Primul strat (64)** acÈ›ioneazÄƒ ca un extractor de secvenÈ›e brute (identificÄƒ periodicitatea diurnÄƒ simplÄƒ), Ã®n timp ce **al doilea strat (32)** modeleazÄƒ interacÈ›iunile neliniare Ã®ntre parametri (ex: corelaÈ›ia Ã®ntÃ¢rziatÄƒ dintre scÄƒderea presiunii È™i intensificarea vÃ¢ntului). O reÈ›ea "shallow" (un singur strat) nu ar fi avut capacitatea de abstracÈ›ie necesarÄƒ pentru prognoza multi-variatÄƒ. |
| **Input window**        | **24 ore** (Timesteps)                         | Dimensiunea ferestrei de intrare $T=24$ a fost aleasÄƒ pe baza autocorelaÈ›iei datelor meteorologice. Deoarece temperatura È™i umiditatea urmeazÄƒ un ciclu solar de 24 de ore, reÈ›eaua are nevoie de o secvenÈ›Äƒ completÄƒ zi-noapte pentru a distinge Ã®ntre o scÄƒdere de temperaturÄƒ cauzatÄƒ de apusul soarelui (fenomen normal) È™i una cauzatÄƒ de un front atmosferic rece (fenomen meteo).                                                                                                                          |
| **Optimizator**         | **Adam** (`lr=0.001`)                          | Algoritmul *Adaptive Moment Estimation* este esenÈ›ial pentru acest dataset heterogen. Deoarece intrÄƒrile au scÄƒri diferite (chiar È™i dupÄƒ normalizare, distribuÈ›iile difera), Adam ajusteazÄƒ rata de Ã®nvÄƒÈ›are individual pentru fiecare greutate. Acest lucru previne ca gradientul sÄƒ fie dominat de parametrii cu varianÈ›Äƒ mare (cum ar fi presiunea) Ã®n detrimentul celor cu varianÈ›Äƒ micÄƒ dar criticÄƒ (precipitaÈ›iile).                                                                                       |
| **FuncÈ›ie de pierdere** | **MSE** (Mean Squared Error)                   | Ãn contextul detecÈ›iei fenomenelor extreme ("Black Swan"), MSE este superior MAE. Eroarea pÄƒtraticÄƒ penalizeazÄƒ disproporÈ›ionat de mult abaterile mari. De exemplu, dacÄƒ modelul rateazÄƒ un vÃ¢rf de caniculÄƒ (44Â°C vs 34Â°C), penalizarea este masivÄƒ ($10^2 = 100$), forÈ›Ã¢nd reÈ›eaua sÄƒ prioritizeze Ã®nvÄƒÈ›area acestor evenimente rare Ã®n detrimentul reducerii erorii pe vreme calmÄƒ.                                                                                                                            |
| **Batch Size**          | **64**                                         | AvÃ¢nd un set de date de antrenare de aprox. 68.000 de eÈ™antioane, un batch de 64 oferÄƒ un estimator al gradientului cu varianÈ›Äƒ redusÄƒ, permiÈ›Ã¢nd o convergenÈ›Äƒ stabilÄƒ. Dimensiunea este suficientÄƒ pentru a vectoriza eficient calculele matriciale pe GPU, dar suficient de micÄƒ pentru a evita minimele locale plate ("sharp minima") asociate cu batch-urile foarte mari.                                                                                                                                    |
| **Regularizare**        | **Dropout (0.2)**                              | Aplicat pentru a contracara efectul de *overfitting* pe datele sintetice. Deoarece 40% din date sunt generate algoritmic (simulare fizicÄƒ), existÄƒ riscul ca reÈ›eaua sÄƒ Ã®nveÈ›e "formula" generatorului Ã®n loc sÄƒ generalizeze fenomenele. Dezactivarea aleatorie a 20% din conexiuni la fiecare pas de antrenare forÈ›eazÄƒ modelul sÄƒ dezvolte reprezentÄƒri redundante È™i robuste ale stÄƒrii vremii.                                                                                                               |

## 3. Procesul de antrenare È™i strategii de optimizare

Procesul de antrenare a fost monitorizat È™i controlat dinamic pentru a asigura convergenÈ›a optimÄƒ È™i pentru a evita overfitting-ul.

### 3.1 Strategii implementate (Callbacks)
Am utilizat mecanisme automate de control al antrenÄƒrii Ã®n Keras:
1.  **Early stopping:** Monitorizarea metricii `val_loss`. DacÄƒ eroarea pe setul de validare nu scade timp de **5 epoci consecutive**, antrenarea se opreÈ™te automat. Aceasta a prevenit degradarea performanÈ›ei dupÄƒ epoca 15.
2.  **Learning rate scheduler (`ReduceLROnPlateau`):** CÃ¢nd modelul atinge un platou de Ã®nvÄƒÈ›are, rata de Ã®nvÄƒÈ›are este redusÄƒ automat (factor 0.5) pentru a permite o ajustare finÄƒ a greutÄƒÈ›ilor cÄƒtre minimul global.

### 3.2 Augmentarea datelor
Pentru a creÈ™te robusteÈ›ea modelului Ã®n mediu industrial/real, am aplicat tehnici de augmentare specifice seriilor de timp:
* **Injectare zgomot Gaussian:** AdÄƒugarea de variaÈ›ii mici aleatoare pe datele de intrare pentru a simula imprecizia senzorilor hardware.
* **Domain randomization (Black Swan):** Integrarea scenariilor sintetice (caniculÄƒ, furtunÄƒ) a forÈ›at modelul sÄƒ Ã®nveÈ›e regimuri de funcÈ›ionare care nu existau Ã®n istoricul recent.

### 3.3 Dinamica antrenÄƒrii (Loss Curve)
Graficul de mai jos ilustreazÄƒ evoluÈ›ia erorii (MSE) pe seturile de antrenare È™i validare:


![Loss Curve](docs/loss_curve_5_input_parameters.png)

*AnalizÄƒ:* ConvergenÈ›a curbelor (Train vs Validation) demonstreazÄƒ cÄƒ modelul a generalizat corect regulile fizice, fÄƒrÄƒ a suferi de overfitting major (distanÈ›a micÄƒ Ã®ntre curbe).

---

## 4. Evaluarea performanÈ›ei (metrici È™i analizÄƒ)

Evaluarea s-a realizat pe setul de **TEST (2024 luni pare)**, date pe care modelul nu le-a vÄƒzut niciodatÄƒ. Deoarece problema este una de regresie, folosim **R2 Score** (coeficient de determinare) ca echivalent al acurateÈ›ei, È™i **MAE/RMSE** pentru eroarea absolutÄƒ.

### 4.1 Rezultate obÈ›inute

| Parametru        | R2 score (acurateÈ›e) | MAE (eroare absolutÄƒ) | RMSE | Interpretare rezultat                                                             |
|:-----------------|:---------------------|:----------------------|:-----|:----------------------------------------------------------------------------------|
| **TemperaturÄƒ**  | **0.950** (Excelent) | 1.56 Â°C               | 2.15 | Modelul prezice temperatura cu o eroare neglijabilÄƒ pentru uz general.            |
| **Presiune**     | **0.946** (Excelent) | 1.26 hPa              | 1.75 | CapteazÄƒ foarte bine tendinÈ›ele barometrice, esenÈ›ial pentru prognoza furtunilor. |
| **Umiditate**    | **0.794** (Bun)      | 7.07 %                | 9.48 | PerformanÈ›Äƒ solidÄƒ, corelatÄƒ invers cu temperatura.                               |
| **VÃ¢nt**         | 0.316 (Slab)         | 0.92 m/s              | 1.23 | VÃ¢ntul la rafalÄƒ este haotic; modelul prinde media, dar rateazÄƒ extremele locale. |
| **PrecipitaÈ›ii** | 0.073 (Critic)       | 0.09 mm               | 0.27 | Afectat de problema "Zero-Inflation" (prea multe zile fÄƒrÄƒ ploaie).               |

### 4.2 Analiza erorilor È™i limitÄƒri (context industrial)
AnalizÃ¢nd rezultatele, am identificat comportamente specifice senzorilor È™i mediului:
1.  **Precizie vs. Haos:** Parametrii inerÈ›iali (TemperaturÄƒ, Presiune) sunt prezisi cu o precizie de **>94%**, depÄƒÈ™ind cerinÈ›a minimÄƒ de 70%. Aceasta valideazÄƒ arhitectura pentru monitorizare standard.
2.  **Problema precipitaÈ›iilor (Zero-Inflation):** Deoarece ploaia este un eveniment rar (sparse data), reÈ›eaua tinde sÄƒ prezicÄƒ valori apropiate de 0 pentru a minimiza MSE-ul global. Pentru o detecÈ›ie mai bunÄƒ, ar fi necesarÄƒ transformarea problemei Ã®n Clasificare (PlouÄƒ/Nu PlouÄƒ).
3.  **VÃ¢ntul:** DeÈ™i R2 este mic, eroarea absolutÄƒ (MAE) este sub 1 m/s, ceea ce este acceptabil pentru aplicaÈ›ii non-critice.

**Vizualizare comparativÄƒ (real vs predicÈ›ie):**
Graficul de mai jos aratÄƒ suprapunerea predicÈ›iilor (roÈ™u) peste datele reale (albastru) pentru un eÈ™antion din setul de test.


![Prediction Plot](docs/prediction_plot_5_input_parameters.png)

---

## 5. Integrarea Ã®n aplicaÈ›ie È™i livrabile

Ca pas final al Etapei 5, modelul antrenat a fost integrat complet Ã®n ecosistemul software.

### 5.1 Status integrare
* [x] **Salvare model:** Modelul final a fost exportat Ã®n formatul standard `models/trained_model_5_input_parameters.keras`.
* [x] **Backend UI:** Dashboard-ul (`src/app/dashboard.py`) Ã®ncarcÄƒ acum acest fiÈ™ier la pornire, Ã®nlocuind modelul dummy din etapa anterioarÄƒ.
* [x] **InferenÈ›Äƒ realÄƒ:** AplicaÈ›ia preia datele de intrare, le normalizeazÄƒ folosind scaler-ul salvat È™i afiÈ™eazÄƒ prognoza denormalizatÄƒ (Ã®n unitÄƒÈ›i reale: Â°C, hPa).

### 5.2 DemonstraÈ›ie funcÈ›ionalitate
Captura de ecran de mai jos confirmÄƒ funcÈ›ionarea modelului Ã®n interfaÈ›a graficÄƒ, realizÃ¢nd o inferenÈ›Äƒ pe date reale:

![Inference Real](docs/screenshots/dashboard_romania_2.png)
*(FigurÄƒ: Prognoza live pe BucureÈ™ti.)*

### 5.3. Actualizarea orchestratorului general (`main.py`)

Pentru a gestiona complexitatea crescutÄƒ a proiectului (date sintetice, preprocesare, antrenare, evaluare), am refactorizat complet fiÈ™ierul `main.py`. Acesta nu mai este un simplu script de test, ci a devenit un **orchestrator inteligent (smart pipeline controller)** care gestioneazÄƒ Ã®ntregul ciclu de viaÈ›Äƒ al proiectului.

**Caracteristici cheie implementate:**

1.  **ExecuÈ›ie inteligentÄƒ (idempotency):**
    * Scriptul verificÄƒ existenÈ›a artefactelor Ã®nainte de execuÈ›ie. De exemplu, dacÄƒ modelul antrenat (`trained_model.keras`) existÄƒ deja, pasul de antrenare este sÄƒrit automat pentru a economisi timp.
    * DacÄƒ datele brute existÄƒ, nu le mai descarcÄƒ de pe Open-Meteo.

2.  **Integritatea pipeline-ului:**
    * AsigurÄƒ ordinea strictÄƒ a operaÈ›iilor: *AchiziÈ›ie $\rightarrow$ Generare SinteticÄƒ $\rightarrow$ Preprocesare $\rightarrow$ Antrenare $\rightarrow$ Evaluare*.
    * GaranteazÄƒ cÄƒ fiÈ™ierul critic `preprocessing_params.pkl` (scaler-ul) este generat Ã®nainte de a porni Dashboard-ul, asigurÃ¢nd compatibilitatea cu modul Live ESP32.

3.  **Control prin linie de comandÄƒ (CLI):**
    * Am implementat argumente (`flags`) care permit utilizatorului sÄƒ forÈ›eze re-execuÈ›ia anumitor etape (ex: `--force-train` pentru a re-antrena modelul de la zero, chiar dacÄƒ existÄƒ unul salvat).

**Logica de execuÈ›ie a orchestratorului:**
```python
# Exemplu simplificat din main.py
if args.force_train or not check_artifact(model_path, "Trained Model"):
    print(f"Training the LSTM model ({config.EPOCHS} epochs)...")
    train_pipeline()
else:
    print("Trained model found. Use --force-train to retrain.")
```

```
## PregÄƒtire Date pentru Antrenare

Deoarece Ã®n Etapa 4 am integrat datele sintetice ("Black Swan"), dataset-ul de antrenare a suferit modificÄƒri structurale majore (creÈ™terea volumului cu >40%). Pentru a asigura consistenÈ›a antrenÄƒrii, am refÄƒcut preprocesarea pe setul hibrid.

### 1. Regenerarea È™i Combinarea Datelor
Am utilizat generatorul sintetic pentru a crea fiÈ™ierul consolidat `hybrid_dataset.csv`.

```bash
# ComandÄƒ rulatÄƒ pentru generarea dataset-ului hibrid:
python src/data_acquisition/synthetic_generator.py

```

*Rezultat:* FiÈ™ierul `data/generated/hybrid_dataset.csv` conÈ›ine acum atÃ¢t istoricul real (2020-2024), cÃ¢t È™i scenariile extreme (CaniculÄƒ, FurtunÄƒ, ÃngheÈ›).

### 2\. Preprocesarea CompletÄƒ (RefÄƒcutÄƒ)

Am rulat scriptul de splitting È™i scalare pe noul dataset hibrid pentru a actualiza parametrii scaler-ului (MinMax) astfel Ã®ncÃ¢t sÄƒ includÄƒ È™i valorile extreme generate (ex: temperaturi de 44Â°C, care depÄƒÈ™esc maximul istoric).

Bash

```
# ComandÄƒ rulatÄƒ pentru preprocesare:
python src/preprocessing/split_data.py

```

**VerificÄƒri efectuate:**

-   [x] **ConsistenÈ›a Scaler-ului:** FiÈ™ierul `config/preprocessing_params.pkl` a fost suprascris cu noile limite [Min, Max] ale setului hibrid.

-   [x] **Stratificare TemporalÄƒ:**

    -   `data/train/train.csv`: ConÈ›ine date istorice (2020-2023) + **Toate datele sintetice**.

    -   `data/validation/validation.csv`: Date reale 2024 (Luni Impare) - nealterate sintetic.

    -   `data/test/test.csv`: Date reale 2024 (Luni Pare) - nealterate sintetic.

    -   *Motiv:* ValidÄƒm È™i testÄƒm modelul pe date exclusiv reale pentru a mÄƒsura performanÈ›a Ã®n condiÈ›ii naturale, dar antrenÄƒm pe date augmentate pentru robusteÈ›e.

**Verificare rapidÄƒ a volumului de date:**

Python

```
import pandas as pd
train = pd.read_csv('data/train/train.csv')
print(f"Total Train Samples: {len(train)}")
# Output aÈ™teptat: ~60,000+ (Include cele 25,000 sintetice)
``````

---

## 6. InstrucÈ›iuni de rulare

Proiectul este conceput modular, avÃ¢nd douÄƒ componente majore: **pipeline-ul de backend** (gestionat de orchestratorul `main.py`) È™i **interfaÈ›a graficÄƒ** (gestionatÄƒ de framework-ul Streamlit).

### 6.1 Rularea pipeline-ului de date È™i antrenare

Pentru a parcurge fluxul complet (DescÄƒrcare $\rightarrow$ Generare Black Swan $\rightarrow$ Antrenare $\rightarrow$ Evaluare), se ruleazÄƒ o singurÄƒ comandÄƒ din rÄƒdÄƒcina proiectului. Orchestratorul va decide inteligent ce paÈ™i trebuie executaÈ›i.

**A. Rulare standard (smart mode):**
AceastÄƒ comandÄƒ va executa doar paÈ™ii necesari (care lipsesc). De exemplu, dacÄƒ modelul este deja antrenat, nu Ã®l va antrena din nou.
```bash
python main.py
```

**B. Rulare forÈ›atÄƒ (full retrain):** DacÄƒ doriÈ›i sÄƒ regeneraÈ›i datele sintetice È™i sÄƒ re-antrenaÈ›i modelul de la zero (util pentru a testa modificÄƒri de arhitecturÄƒ sau hiperparametri):
```bash
python main.py --force-data --force-train
```

**C. Rulare fÄƒrÄƒ evaluare:** Pentru o execuÈ›ie rapidÄƒ, doar pentru antrenare, fÄƒrÄƒ calculul metricilor finale:
```bash
python main.py --force-train --skip-eval
```

**Output aÈ™teptat Ã®n consolÄƒ:**
```text
============================================================
   SIA-METEO: INTELLIGENT PIPELINE ORCHESTRATOR
============================================================

>>> Phase 1: Data acquisition
Raw data already exists. Use --force-data to overwrite.
------------------------------
>>> Phase 2: Synthetic data augmentation
Hybrid dataset already exists.
------------------------------
>>> Phase 3: Preprocessing and normalization
Data is already processed and normalized.
------------------------------
>>> Phase 4: Model training (LSTM)
Training the LSTM model (50 epochs)...
... [Keras Training Logs: Epoch 1/50 ... Epoch 15/50] ...
Model saved to models/trained_model.keras
------------------------------
>>> Phase 5: Evaluation and metrics
Running evaluation on test set (2024)...
Test MSE: 0.0024 | R2 Score: 0.94
------------------------------

============================================================
   âœ… PIPELINE COMPLETE. SYSTEM READY FOR LIVE MODE.
============================================================
```

### 6.2 Rularea interfeÈ›ei grafice (UI)

DupÄƒ ce pipeline-ul a rulat cu succes È™i a generat fiÈ™ierele critice (`trained_model.keras` È™i `preprocessing_params.pkl`), puteÈ›i lansa dashboard-ul interactiv.

**ComandÄƒ:**
```bash
streamlit run src/app/dashboard.py
```

**Comportament:** AplicaÈ›ia se va deschide automat Ã®n browserul implicit la adresa `http://localhost:8501`. DacÄƒ rulaÈ›i pe un server remote, accesaÈ›i IP-ul serverului la portul 8501.

**Module disponibile Ã®n UI:**

1.  **Tab-ul "Live RomÃ¢nia":** Vizualizarea prognozei pe date istorice reale.

2.  **Tab-ul "Simulator":** Testarea reacÈ›iei modelului la scenarii manuale (ex: introducerea bruscÄƒ a unei presiuni de 980 hPa).

3.  **Tab-ul "Monitor ESP32":** InterfaÈ›a de conectare pentru senzorii hardware (necesitÄƒ conexiune serialÄƒ/MQTT activÄƒ).

---

## 7. Structura repository-ului la finalul etapei 5

```text
Proiect_ReteleNeuronale_Meteo/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ preprocessing_params.pkl   # FiÈ™ierul de denormalizare a datelor
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ generated/                 # Date sintetice (extreme) + Dataset hibrid
â”‚   â”‚   â”œâ”€â”€ hybrid_dataset.csv
â”‚   â”‚   â””â”€â”€ synthetic_extremes.csv
â”‚   â”œâ”€â”€ raw/                       # Date brute
â”‚   â”‚   â””â”€â”€ weather_history_raw.csv
â”‚   â”œâ”€â”€ test/                      # Set de testare (2024 luni pare)
â”‚   â”‚   â””â”€â”€ test.csv 
â”‚   â”œâ”€â”€ train/                     # Set de instruire (2020-2023)
â”‚   â”‚   â””â”€â”€ train.csv 
â”‚   â””â”€â”€ validation/                # Set de validare (2024 luni impare)
â”‚       â””â”€â”€ validation.csv 
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ screenshots/               # FiÈ™ier pentru capturile de ecran ale UI-ului
â”‚   â”‚   â”œâ”€â”€ dashboard_liveESP.png
â”‚   â”‚   â”œâ”€â”€ dashboard_romania_1.png
â”‚   â”‚   â”œâ”€â”€ dashboard_romania_1.png
â”‚   â”‚   â”œâ”€â”€ dashboard_romania_1.png
â”‚   â”‚   â””â”€â”€ dashboard_simulation.png
â”‚   â”œâ”€â”€ distribution_comparison.png # DistribuÈ›ia temperaturilor Ã®n setul de date hibrid (etapa 4)
â”‚   â”œâ”€â”€ eda_correlation.png        # Matricea de corelaÈ›ie
â”‚   â”œâ”€â”€ eda_distribution.png       # DistribuÈ›ia datelor
â”‚   â”œâ”€â”€ eda_outliers.png           # Identificarea outlier-ilor
â”‚   â”œâ”€â”€ loss_curve.png             # Graficul de antrenare a modelului
â”‚   â”œâ”€â”€ prediction_plot.png        # Graficele de predicÈ›ie pentru fiecare parametru
â”‚   â”œâ”€â”€ state-machine-RN.drawio    # Diagrama state-machine a sistemului (fiÈ™ier .drawio)
â”‚   â””â”€â”€ state-machine-RN.png       # Diagrama state-machine a sistemului 
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained_model.keras        # Model antrenat corespunzÄƒtor
â”‚   â””â”€â”€ untrained_model.keras      # Model antrenat doar pentru demo (etapa 4)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ test_metrics.json          # Statisticile antrenÄƒrilor
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                       # Script UI
â”‚   â”‚   â””â”€â”€ dashboard.py
â”‚   â”œâ”€â”€ data_acquisition/          # Script descÄƒrcare, generare È™i impachetare hibridÄƒ
â”‚   â”‚   â”œâ”€â”€ __init__.py            # IniÈ›ializarea pachetului
â”‚   â”‚   â”œâ”€â”€ data_loader.py         # DescarcÄƒ datele istorice brute de la API-ul Open-Meteo
â”‚   â”‚   â””â”€â”€ synthetic_generator.py # GenereazÄƒ evenimente â€Black Swanâ€ È™i face dateset-ul hybrid
â”‚   â”œâ”€â”€ docs_generators/           # Generatoare de documentaÈ›ii
â”‚   â”‚   â”œâ”€â”€ __init__.py            # IniÈ›ializarea pachetului
â”‚   â”‚   â”œâ”€â”€ generate_docs.py       # GenereazÄƒ statistici pe baza setului hibrid de date
â”‚   â”‚   â””â”€â”€ generate_eda.py        # GenereazÄƒ statistici pe baza setului brut de date
â”‚   â”œâ”€â”€ neural_network/            # Scripturi pentru modelul neuronal
â”‚   â”‚   â”œâ”€â”€ data_generator.py      # Transformarea datelor din 2D Ã®n 3D perestre secvenÈ›iale
â”‚   â”‚   â”œâ”€â”€ evaluate.py            # Testarea modelului si formarea statisticilor
â”‚   â”‚   â”œâ”€â”€ model.py               # Arhitectura reÈ›elei neuronale (fazÄƒ incipientÄƒ)
â”‚   â”‚   â””â”€â”€ train.py               # Antrenarea modelului (fazÄƒ incipientÄƒ)
â”‚   â”œâ”€â”€ preprocessing/             # Scripturi de split È™i normalizare
â”‚   â”‚   â”œâ”€â”€ __init__.py            # IniÈ›ializarea pachetului
â”‚   â”‚   â””â”€â”€ split_data.py          # Ãmparte datele (Train/Val/Test) È™i aplicÄƒ normalizarea MinMax
â”‚   â”œâ”€â”€ __init__.py                # IniÈ›ializarea pachetului
â”‚   â””â”€â”€ config.py                  # FiÈ™ier cu date de configurare È™i constante
â”œâ”€â”€ .gitignore                     # GestioneazÄƒ fiÈ™ierele ce nu trebuie postate pe GitHub
â”œâ”€â”€ main.py                        # Orchestrator principal
â”œâ”€â”€ README.md
â”œâ”€â”€ README_Etapa3_Analiza_Date.md
â”œâ”€â”€ README_Etapa4_Arhitectura_SIA.md
â”œâ”€â”€ README_Etapa5_Antrenare_RN.md  # Acest fiÈ™ier
â””â”€â”€ requirements.txt               # DependenÈ›e Python