# ğŸ“˜ README â€“ Etapa 3: Analiza È™i PregÄƒtirea Setului de Date

**Disciplina:** ReÈ›ele Neuronale <br />
**InstituÈ›ie:** POLITEHNICA BucureÈ™ti â€“ FIIR <br />
**Student:** PetruÈ›iu Darius-Simion <br />
**Link Repository GitHub:** https://github.com/PetrutiuDarius/Proiect_ReteleNeuronale_Meteo.git <br />
**Data:** 04.11.2025 <br />

---

## Introducere

Acest document descrie activitÄƒÈ›ile realizate Ã®n **Etapa 3**, Ã®n care s-a analizat È™i preprocesat setul de date necesar proiectului â€Prognoza Meteoâ€. Scopul etapei a fost transformarea datelor meteorologice brute Ã®ntr-un format optim pentru reÈ›ele neuronale (serii temporale normalizate), rezolvÃ¢nd totodatÄƒ problema lipsei de fenomene extreme din datele istorice.

---

## 1. Structura Repository-ului Github (Versiunea Etapei 3)

```text
Proiect_ReteleNeuronale_Meteo/
â”œâ”€â”€ README.md
â”œâ”€â”€ etapa3_analiza_date.md         # Acest fiÈ™ier
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Date brute (istoric Open-Meteo)
â”‚   â”œâ”€â”€ generated/                 # Date sintetice (Extreme) È™i Dataset Hibrid
â”‚   â”œâ”€â”€ train/                     # Set de instruire (2020-2023 + Sintetic)
â”‚   â”œâ”€â”€ validation/                # Set de validare (2024 Luni Impare)
â”‚   â””â”€â”€ test/                      # Set de testare (2024 Luni Pare)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_acquisition/          # Scripturi descÄƒrcare È™i generare date
â”‚   â””â”€â”€ processing/                # Scripturi de split È™i normalizare
â”œâ”€â”€ requirements.txt               # DependenÈ›e Python
```

## 2. Descrierea Setului de Date

### 2.1 Sursa datelor
* **Origine:** API Open-Meteo (Historical Weather) + Generator Sintetic Propriu.
* **Modul de achiziÈ›ie:**
  * DescarcÄƒ date reale (API Request) via `src/data_acquisition/data_loader.py`.
  * Generare programaticÄƒ (Algoritm statistic) via `src/data_acquisition/synthetic_generator.py`.
* **Perioada:** 01.01.2020 â€“ 31.12.2024.

### 2.2 Caracteristicile dataset-ului
* **NumÄƒr total de observaÈ›ii:** ~60,000 ore (din care ~25,000 simulate).
* **NumÄƒr de caracteristici (features):** 4 (TemperaturÄƒ, Umiditate, Presiune, VÃ¢nt).
* **Tipuri de date:** Numerice (Serii Temporale Multivariate).
* **Format fiÈ™iere:** CSV.

### 2.3 Descrierea fiecÄƒrei caracteristici

| CaracteristicÄƒ | Tip | Unitate | Descriere | Domeniu valori (Real+Simulat) |
|---|---|---|---|---|
| temperature | numeric | Â°C | Temperatura aerului la 2m | -15.0 ... +44.0 |
| humidity | numeric | % | Umiditatea relativÄƒ | 20.0 ... 100.0 |
| pressure | numeric | hPa | Presiunea atmosfericÄƒ | 980.0 ... 1030.0 |
| wind_speed | numeric | m/s | Viteza vÃ¢ntului la 10m | 0.0 ... 30.0 |

## 3. Analiza Exploratorie a Datelor (EDA)

### 3.1 Statistici descriptive aplicate
* S-a analizat distribuÈ›ia datelor reale pe perioada 2020-2024.
* **Concluzie:** Datele reale au o distribuÈ›ie normalÄƒ, dar lipsesc valorile extreme critice pentru siguranÈ›Äƒ (ex: nu existÄƒ temperaturi > 42Â°C sau vÃ¢nt > 25m/s Ã®n istoric).

### 3.2 Analiza calitÄƒÈ›ii datelor
* **Valori lipsÄƒ:** Open-Meteo furnizeazÄƒ date complete. Eventualele goluri minore sunt tratate prin interpolare liniarÄƒ (`method='time'`).
* **ConsistenÈ›Äƒ:** S-a verificat cronologia timestamp-urilor.

### 3.3 Probleme identificate
* **ProblemÄƒ:** "Imbalanced Dataset" Ã®n ceea ce priveÈ™te fenomenele extreme. Evenimentele de tip "FurtunÄƒ violentÄƒ" sau "CaniculÄƒ extremÄƒ" reprezentau < 0.1% din datele reale.
* **SoluÈ›ie:** Augmentarea setului de date prin generarea a 25,000 de ore de date sintetice ("Black Swan events") care au fost adÄƒugate la setul de antrenare.

## 4. Preprocesarea Datelor

### 4.1 CurÄƒÈ›area È™i Transformarea
Procesul este automatizat Ã®n `src/processing/split_data.py`:
* **Imputare:** Interpolare liniarÄƒ pentru continuitate temporalÄƒ.
* **Normalizare:** S-a utilizat **MinMax Scaler** pentru a aduce toate valorile Ã®n intervalul `[0, 1]`.
* **NotÄƒ:** Scalerul a fost antrenat (`.fit`) **DOAR** pe setul de antrenare pentru a evita *Data Leakage*, È™i apoi aplicat pe validare È™i test.

### 4.2 Structurarea seturilor de date
S-a ales o Ã®mpÄƒrÈ›ire cronologicÄƒ modificatÄƒ (nu aleatorie/stratificatÄƒ), specificÄƒ seriilor temporale:
* **Train (70%):** Anii 2020-2023 (Real) + Toate Datele Simulate.
* **Validation (15%):** Anul 2024 (Luni Impare: Ian, Mar, ...).
* **Test (15%):** Anul 2024 (Luni Pare: Feb, Apr, ...).

### 4.3 Salvarea rezultatelor
* FiÈ™ierele CSV finale sunt salvate Ã®n `data/train/`, `data/validation/`, `data/test/`.
* Obiectul Scaler este salvat Ã®n `data/scalers/minmax_scaler.pkl` pentru a putea fi folosit ulterior la denormalizarea predicÈ›iilor.

## 5. Stare EtapÄƒ
- [x] StructurÄƒ repository configuratÄƒ
- [x] Dataset analizat (EDA realizatÄƒ)
- [x] Date generate sintetic (rezolvare lipsÄƒ extreme)
- [x] Date preprocesate È™i normalizate
- [x] Seturi train/validation/test generate
- [x] DocumentaÈ›ie actualizatÄƒ